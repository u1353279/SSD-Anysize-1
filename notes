1. Something is going on with the evaluation - very apparent when
using the license plate dataset. Sometimes the expected mAP values 
are "swapped", ie: sometimes the license plate gets a high score
while sometimes the vehicle gets a high score (after the first epoch).
Since the two values are distinct and seem to randomly "swap" 
this suggests that there's an issue in the eval function that needs
to be rectified.

To repro, just do a few 1-epoch runs and watch the expected mAP values 
jump from one class to another. See below to check this in action


2. Benchmark accuracies on datasets for all models @ 300x300, 10 epochs 

- license plate dataset
mnetv2: 
(run 1) {'license_plate': 0.146, 'vehicle': 0.693} (mAP): 0.420

rnet18:
(run 1) {'license_plate': 0.181, 'vehicle': 0.792} (mAP): 0.487

rnet34:
(run 1) {'license_plate': 0.838, 'vehicle': 0.207} (mAP): 0.523 (see how the mAP's seem off???)
(run 2) {'license_plate': 0.19206659495830536, 'vehicle': 0.7975136637687683} (mAP): 0.495

^ see?? I bet that's because the order in which we see the annotations and thus create the 
classes file isn't consistent. I'll have to sort
